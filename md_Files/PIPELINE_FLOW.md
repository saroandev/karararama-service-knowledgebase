# RAG Pipeline Veri AkÄ±ÅŸ DetaylarÄ±

Bu dokÃ¼man, kullanÄ±cÄ±nÄ±n bir soru sormasÄ±ndan cevap almasÄ±na kadar geÃ§en sÃ¼redeki veri akÄ±ÅŸÄ±nÄ± adÄ±m adÄ±m aÃ§Ä±klar. AyrÄ±ca anlÄ±k belge yÃ¼kleme (ingestion) sÃ¼recini de detaylandÄ±rÄ±r.

## ğŸ”„ Genel Pipeline AkÄ±ÅŸÄ±

```mermaid
graph TD
    A[KullanÄ±cÄ± Sorusu] --> B[Query Endpoint]
    B --> C[Soru Embedding'e DÃ¶nÃ¼ÅŸtÃ¼rme]
    C --> D[Milvus Vector Search]
    D --> E[En YakÄ±n Chunk'larÄ± Bulma]
    E --> F[Metadata ile ZenginleÅŸtirme]
    F --> G[Context OluÅŸturma]
    G --> H[OpenAI LLM'e GÃ¶nderme]
    H --> I[Cevap Ãœretimi]
    I --> J[Kaynak Bilgileri Ekleme]
    J --> K[JSON Response]
```

## 1ï¸âƒ£ KullanÄ±cÄ± Sorusu GeliÅŸi

### HTTP Request
```http
POST /query HTTP/1.1
Content-Type: application/json

{
    "question": "HarcÄ±rahlarÄ±n Ã¶deme ÅŸartlarÄ± nelerdir?",
    "top_k": 3,
    "document_id": null
}
```

### Kod Yolu: `production_server.py:255`
```python
@app.post("/query", response_model=QueryResponse)
async def query_documents(request: QueryRequest):
    start_time = datetime.datetime.now()
    logger.info(f"Query: {request.question}")
```

---

## 2ï¸âƒ£ Soru Embedding'e DÃ¶nÃ¼ÅŸtÃ¼rme

### OpenAI API Ã‡aÄŸrÄ±sÄ±
```python
# production_server.py:274-278
query_response = client.embeddings.create(
    model='text-embedding-3-small',
    input=request.question
)
query_embedding = query_response.data[0].embedding
```

### Veri DÃ¶nÃ¼ÅŸÃ¼mÃ¼
- **GiriÅŸ**: `"HarcÄ±rahlarÄ±n Ã¶deme ÅŸartlarÄ± nelerdir?"` (String)
- **Ã‡Ä±kÄ±ÅŸ**: `[0.123, -0.456, 0.789, ...]` (1536 boyutlu float array)
- **API SÃ¼resi**: ~200-500ms

---

## 3ï¸âƒ£ Milvus Vector Search

### VeritabanÄ± Sorgusu
```python
# production_server.py:286-293
search_results = collection.search(
    [query_embedding],           # Query vector
    'embedding',                 # Field adÄ±
    {'metric_type': 'COSINE'},   # Similarity metric
    limit=request.top_k,         # KaÃ§ sonuÃ§
    expr=expr,                   # Filtreler (opsiyonel)
    output_fields=['document_id', 'document_title', 'text', 'page_num', 'created_at']
)
```

### Milvus'ta Ne Oluyor?
1. **Index KullanÄ±mÄ±**: IVF_FLAT index ile hÄ±zlÄ± arama
2. **Cosine Similarity**: Query vector ile tÃ¼m chunk vectorleri karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r
3. **Top-K Selection**: En yÃ¼ksek similarity score'lu K adet chunk seÃ§ilir
4. **Metadata Retrieval**: SeÃ§ilen chunk'lar iÃ§in metadata getirilir

### Ã–rnek Milvus Sonucu
```json
[
    {
        "score": 0.483,
        "entity": {
            "document_id": "doc_06054c7f7733730e",
            "document_title": "HARCIRAH TÃœZÃœÄÃœ",
            "text": "Posta gezici personeline, tren, vapur...",
            "page_num": 1,
            "created_at": "2025-09-05T11:43:29.597080"
        }
    },
    {
        "score": 0.458,
        "entity": {...}
    }
]
```

---

## 4ï¸âƒ£ Context Assembly (BaÄŸlam OluÅŸturma)

### Sources Array HazÄ±rlama
```python
# production_server.py:304-326
sources = []
context_parts = []

for i, result in enumerate(search_results[0]):
    score = result.score
    doc_id = result.entity.get('document_id')
    text = result.entity.get('text')
    
    sources.append({
        "rank": i + 1,
        "score": round(score, 3),
        "document_id": doc_id,
        "text_preview": text[:200] + "...",
        # ... diÄŸer metadata
    })
    
    context_parts.append(f"[Kaynak {i+1} - Sayfa {page_num}]: {text}")
```

### Context String OluÅŸturma
```python
context = "\n\n".join(context_parts)
```

### Ã–rnek Context
```
[Kaynak 1 - Sayfa 1]: Posta gezici personeline, tren, vapur, karayollarÄ± ve havayollarÄ± araÃ§larÄ±nda gezici olarak posta gÃ¶tÃ¼rÃ¼p getiren personele ve bunlara eÅŸlik ettirilen taÅŸÄ±cÄ±lara, Ã§Ä±kÄ±ÅŸ yerlerinden vasÄ±talarÄ±n hareket saatinden itibaren...

[Kaynak 2 - Sayfa 1]: Gezici personelin son varÄ±ÅŸ yerlerinde dÃ¶nÃ¼ÅŸ servisine baÅŸlamak Ã¼zere beklemekle geÃ§irdikleri sÃ¼re, harcÄ±rah alma sÃ¼resine dahil edilir...
```

---

## 5ï¸âƒ£ OpenAI LLM Ã‡aÄŸrÄ±sÄ±

### Chat Completion Request
```python
# production_server.py:330-351
chat_response = client.chat.completions.create(
    model='gpt-4o-mini',
    messages=[
        {
            "role": "system", 
            "content": """Sen bir RAG asistanÄ±sÄ±n. 
            Verilen kaynak belgelerden faydalanarak cevaplÄ±yorsun.
            CevabÄ±nÄ± verirken kaynak numaralarÄ±nÄ± belirt."""
        },
        {
            "role": "user",
            "content": f"""Kaynak Belgeler:
{context}

Soru: {request.question}

LÃ¼tfen bu soruya kaynak belgelere dayanarak cevap ver."""
        }
    ],
    max_tokens=500
)
```

### LLM Processing
1. **System Prompt**: RAG davranÄ±ÅŸ talimatlarÄ±
2. **Context Injection**: Bulunan chunk'lar prompt'a eklenir
3. **Question**: KullanÄ±cÄ±nÄ±n orijinal sorusu
4. **Generation**: GPT-4o-mini model cevap Ã¼retir
5. **Citation**: Model otomatik olarak `[Kaynak X]` referanslarÄ± ekler

---

## 6ï¸âƒ£ Response Assembly

### Final Response OluÅŸturma
```python
# production_server.py:358-363
return QueryResponse(
    answer=answer,                    # LLM'den gelen cevap
    sources=sources,                  # Kaynak bilgileri array'i
    processing_time=processing_time,  # Toplam sÃ¼re
    model_used="gpt-4o-mini"         # KullanÄ±lan model
)
```

### Ã–rnek Final JSON Response
```json
{
    "answer": "HarcÄ±rahlarÄ±n Ã¶deme ÅŸartlarÄ± aÅŸaÄŸÄ±daki gibidir:\n\n1. Posta gezici personeline, tren, vapur, karayollarÄ± ve havayollarÄ± araÃ§larÄ±nda gezici olarak posta gÃ¶tÃ¼rÃ¼p getiren personele... [Kaynak 1] [Kaynak 2]",
    "sources": [
        {
            "rank": 1,
            "score": 0.483,
            "document_id": "doc_06054c7f7733730e",
            "document_title": "HARCIRAH TÃœZÃœÄÃœ",
            "page_number": 1,
            "text_preview": "POSTA GEZÄ°CÄ° PERSONELÄ°NE VERÄ°LECEK\nHARCIRAH TÃœZÃœÄÃœ...",
            "created_at": "2025-09-05T11:43:29.597080"
        }
    ],
    "processing_time": 4.558254,
    "model_used": "gpt-4o-mini"
}
```

---

## âš¡ Performans Metrikleri

### Tipik Ä°ÅŸlem SÃ¼releri
| AdÄ±m | SÃ¼re | AÃ§Ä±klama |
|------|------|----------|
| **Embedding Generation** | 200-500ms | OpenAI API call |
| **Vector Search** | 10-50ms | Milvus index search |
| **Context Assembly** | 5-10ms | Python string operations |
| **LLM Generation** | 2-5 saniye | GPT-4o-mini response |
| **Response Assembly** | 1-5ms | JSON serialization |
| **TOPLAM** | **3-6 saniye** | End-to-end |

### Log Ã–rneÄŸi
```
INFO:production_server:Query: HarcÄ±rahlarÄ±n Ã¶deme ÅŸartlarÄ± nelerdir?
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"  
INFO:production_server:Query completed in 6.06s
```

---

## ğŸ” Veri YapÄ±larÄ± DetayÄ±

### QueryRequest (GiriÅŸ)
```python
class QueryRequest(BaseModel):
    question: str                           # KullanÄ±cÄ± sorusu
    top_k: int = Field(default=3)          # KaÃ§ chunk getirileceÄŸi
    document_id: Optional[str] = None       # Spesifik dokÃ¼man filtresi
```

### Milvus Search Result (Ara Data)
```python
{
    "score": float,              # Similarity score (0-1)
    "entity": {
        "document_id": str,      # DokÃ¼man ID
        "document_title": str,   # PDF baÅŸlÄ±ÄŸÄ±
        "text": str,            # Chunk tam metni
        "page_num": int,        # Sayfa numarasÄ±
        "chunk_index": int,     # Chunk sÄ±rasÄ±
        "created_at": str       # Ä°ndexlenme tarihi
    }
}
```

### QueryResponse (Ã‡Ä±kÄ±ÅŸ)
```python
class QueryResponse(BaseModel):
    answer: str                    # LLM cevabÄ±
    sources: List[Dict]           # Kaynak bilgileri
    processing_time: float        # Ä°ÅŸlem sÃ¼resi
    model_used: str              # KullanÄ±lan LLM
```

---

## ğŸ”§ Hata DurumlarÄ±

### 1. Embedding API HatasÄ±
```python
# OpenAI API down veya rate limit
except Exception as e:
    raise HTTPException(status_code=500, detail=f"Embedding failed: {str(e)}")
```

### 2. Milvus BaÄŸlantÄ± HatasÄ±
```python
# Milvus service down
except Exception as e:
    raise HTTPException(status_code=503, detail="Vector database unavailable")
```

### 3. SonuÃ§ BulunamadÄ±
```python
if not search_results[0]:
    return QueryResponse(
        answer="Ä°lgili bilgi bulunamadÄ±.",
        sources=[],
        processing_time=0,
        model_used="gpt-4o-mini"
    )
```

### 4. LLM API HatasÄ±
```python
# OpenAI Chat API hatasÄ±
except Exception as e:
    raise HTTPException(status_code=500, detail=f"Answer generation failed: {str(e)}")
```

---

## ğŸ“Š Monitoring NoktalarÄ±

### Key Metrics
- **Embedding Latency**: OpenAI embedding API sÃ¼releri
- **Search Latency**: Milvus vector search sÃ¼releri  
- **Generation Latency**: LLM cevap Ã¼retim sÃ¼releri
- **End-to-End Latency**: Toplam iÅŸlem sÃ¼releri
- **Hit Rate**: BaÅŸarÄ±lÄ± sonuÃ§ bulma oranÄ±
- **Error Rate**: Hata oranlarÄ±

### Logging Points
```python
logger.info(f"Query: {request.question}")                    # Query start
logger.info(f"Found {len(chunks)} relevant chunks")         # Search results
logger.info(f"Query completed in {processing_time:.2f}s")   # Query end
```

Bu akÄ±ÅŸ, her kullanÄ±cÄ± sorgusu iÃ§in tekrarlanÄ±r ve sistem performansÄ± bu adÄ±mlarÄ±n optimizasyonuna baÄŸlÄ±dÄ±r.

---

# ğŸ“„ ANLIK BELGE YÃœKLEME (INGESTION) SÃœRECÄ°

## ğŸ”„ Ingestion Pipeline AkÄ±ÅŸÄ±

```mermaid
graph TD
    A[PDF Upload] --> B[/ingest Endpoint]
    B --> C[File Validation]
    C --> D[PDF Hash Check]
    D --> E{Dokuman Var mÄ±?}
    E -->|Evet| F[Already Exists Response]
    E -->|HayÄ±r| G[PDF Parse]
    G --> H[Text Extraction]
    H --> I[Page-by-Page Chunking]
    I --> J[Embedding Generation]
    J --> K[Milvus Insert]
    K --> L[Collection Reload]
    L --> M[Success Response]
    M --> N[AnÄ±nda Search HazÄ±r!]
```

## ğŸ“‹ AnlÄ±k YÃ¼kleme AdÄ±mlarÄ±

### 1ï¸âƒ£ PDF Upload & Validation

```http
POST /ingest HTTP/1.1
Content-Type: multipart/form-data

file: [PDF Binary Data]
```

**Kod Yolu**: `production_server.py:100-119`

```python
@app.post("/ingest", response_model=IngestResponse)
async def ingest_document(file: UploadFile = File(...)):
    # File type validation
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Only PDF files are supported")
    
    # Read PDF data
    pdf_data = await file.read()
    file_hash = hashlib.md5(pdf_data).hexdigest()
    document_id = f"doc_{file_hash[:16]}"
```

### 2ï¸âƒ£ Duplicate Detection (Hash-based)

```python
# Check if document already exists
search_existing = collection.query(
    expr=f'document_id == "{document_id}"',
    output_fields=['id'],
    limit=1
)

if search_existing:
    return IngestResponse(
        success=False,
        message="Document already exists in database"
    )
```

**AvantajÄ±**: AynÄ± PDF'i tekrar iÅŸlemekten kaÃ§Ä±nÄ±r (performans)

### 3ï¸âƒ£ PDF Parsing & Text Extraction

```python
# 1. PDF Parse using PyMuPDF
from app.parse import PDFParser
parser = PDFParser()
pages, metadata = parser.extract_text_from_pdf(pdf_data)

document_title = metadata.title or file.filename.replace('.pdf', '')
```

**Ä°ÅŸlem**: 
- PyMuPDF ile sayfa sayfa metin Ã§Ä±karma
- Metadata extraction (baÅŸlÄ±k, yazar, tarih)
- OCR desteÄŸi (gÃ¶rÃ¼ntÃ¼lÃ¼ PDF'ler iÃ§in)

### 4ï¸âƒ£ Text Chunking

```python
chunks = []
for i, page in enumerate(pages):
    text = page.text.strip()
    if len(text) > 100:  # Skip very short pages
        chunk_id = f"chunk_{document_id}_{i:04d}_{hash(text[:100]) & 0xffff:04x}"
        chunk = SimpleChunk(
            chunk_id=chunk_id,
            text=text,
            page_number=page.page_number
        )
        chunks.append(chunk)
```

**Strateji**: 
- Sayfa bazlÄ± chunking (basit ama etkili)
- Her chunk benzersiz ID alÄ±r
- Minimum 100 karakter threshold

### 5ï¸âƒ£ Batch Embedding Generation

```python
# OpenAI API calls for each chunk
embeddings = []
for i, chunk in enumerate(chunks):
    response = client.embeddings.create(
        model='text-embedding-3-small',
        input=chunk.text
    )
    embeddings.append(response.data[0].embedding)
    
    if (i + 1) % 5 == 0:
        logger.info(f"Processed {i + 1}/{len(chunks)} chunks")
```

**Performans Optimizasyonu**:
- Batch processing (5'erli gruplar halinde log)
- Progress tracking
- Error handling her chunk iÃ§in

### 6ï¸âƒ£ Milvus Vector Insert

```python
# Prepare batch data for Milvus
data = [
    chunk_ids,           # VARCHAR primary keys
    document_ids,        # VARCHAR document references  
    document_titles,     # VARCHAR document titles
    texts,              # VARCHAR full text content
    embeddings,         # FLOAT_VECTOR(1536) 
    page_nums,          # INT64 page numbers
    chunk_indices,      # INT64 chunk order
    created_ats,        # VARCHAR timestamps
    file_hashes        # VARCHAR file hashes
]

# Single batch insert
insert_result = collection.insert(data)
collection.load()  # CRITICAL: Reload for immediate search
```

**Milvus Schema**:
```python
{
  "collection_name": "rag_production_v1",
  "fields": [
    {"name": "id", "type": "VARCHAR", "is_primary": True},
    {"name": "embedding", "type": "FLOAT_VECTOR", "dim": 1536},
    {"name": "document_id", "type": "VARCHAR"},
    {"name": "document_title", "type": "VARCHAR"},
    {"name": "text", "type": "VARCHAR"},
    {"name": "page_num", "type": "INT64"},
    {"name": "chunk_index", "type": "INT64"},
    {"name": "created_at", "type": "VARCHAR"},
    {"name": "file_hash", "type": "VARCHAR"}
  ]
}
```

### 7ï¸âƒ£ Immediate Availability

```python
collection.load()  # Bu Ã§ok kritik!
```

**ANAHTAR NOKTA**: `collection.load()` Ã§aÄŸrÄ±sÄ± sayesinde yeni eklenen dokÃ¼man **anÄ±nda** arama iÃ§in hazÄ±r hale gelir.

---

## âš¡ Ingestion Performans Metrikleri

| AdÄ±m | Tipik SÃ¼re | AÃ§Ä±klama |
|------|------------|----------|
| **File Upload** | 100-500ms | Network + validation |
| **PDF Parsing** | 1-3 saniye | PyMuPDF processing |  
| **Chunking** | 10-50ms | Text processing |
| **Embedding Generation** | 2-10 saniye | OpenAI API calls (chunk sayÄ±sÄ±na gÃ¶re) |
| **Milvus Insert** | 100-500ms | Batch vector insert |
| **Collection Reload** | 200-1000ms | Index refresh |
| **TOPLAM** | **5-15 saniye** | DokÃ¼man boyutuna gÃ¶re |

### Ã–rnek Log AkÄ±ÅŸÄ±
```
INFO:production_server:Starting ingest for: yeni_dokuman.pdf
INFO:production_server:Document ID: doc_a1b2c3d4e5f6789a, Hash: a1b2c3d4e5f6789a...
INFO:app.parse:Extracted 25 pages from PDF  
INFO:production_server:Parsed 25 pages, title: Yeni DokÃ¼man
INFO:production_server:Created 25 chunks
INFO:production_server:Generating embeddings...
INFO:production_server:Processed 5/25 chunks
INFO:production_server:Processed 10/25 chunks
...
INFO:production_server:Processed 25/25 chunks
INFO:production_server:Inserting to production Milvus...
INFO:production_server:Successfully ingested 25 chunks in 12.34s
```

---

## ğŸ”„ YÃ¼kleme SonrasÄ± AnlÄ±k KullanÄ±m

### Scenario: PDF YÃ¼kle â†’ Hemen Soru Sor

```bash
# 1. PDF yÃ¼kle (12 saniye)
curl -X POST "http://localhost:8090/ingest" \
  -F "file=@yeni_dokuman.pdf"

# Response:
{
  "success": true,
  "document_id": "doc_a1b2c3d4e5f6789a", 
  "chunks_created": 25,
  "processing_time": 12.34,
  "message": "Document successfully ingested with 25 chunks"
}

# 2. Hemen soru sor (4 saniye)
curl -X POST "http://localhost:8090/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "Bu yeni dokÃ¼manda ne anlatÄ±lÄ±yor?"}'

# Response: Yeni PDF'ten cevap gelir!
{
  "answer": "Bu dokÃ¼manda... [Kaynak 1]",
  "sources": [{
    "document_id": "doc_a1b2c3d4e5f6789a",  # Yeni yÃ¼klenen PDF
    "document_title": "Yeni DokÃ¼man",
    "score": 0.85
  }]
}
```

---

## ğŸš€ Real-time Availability Garantisi

### Critical Code Points

1. **Immediate Insert**: 
   ```python
   insert_result = collection.insert(data)  # AnÄ±nda ekle
   ```

2. **Immediate Load**:
   ```python
   collection.load()  # Index'i yenile, arama iÃ§in hazÄ±r hale getir
   ```

3. **No Caching Delays**: Milvus in-memory index kullanÄ±r, gecikme yok

### Test Verification
```python
# Insert sonrasÄ± anÄ±nda search testi
search_results = collection.search(
    [test_embedding],
    'embedding', 
    {'metric_type': 'COSINE'},
    limit=1,
    expr=f'document_id == "{new_document_id}"'
)
assert len(search_results[0]) > 0  # Yeni dokÃ¼man bulunmalÄ±
```

---

## âš ï¸ Ingestion Error Scenarios

### 1. Duplicate Document
```python
if search_existing:
    return IngestResponse(
        success=False,
        message="Document already exists in database"
    )
```

### 2. PDF Parsing Failure
```python
try:
    pages, metadata = parser.extract_text_from_pdf(pdf_data)
except Exception as e:
    return IngestResponse(
        success=False,
        message=f"PDF parsing failed: {str(e)}"
    )
```

### 3. OpenAI API Failure
```python
try:
    response = client.embeddings.create(...)
except Exception as e:
    return IngestResponse(
        success=False,
        message=f"Embedding generation failed: {str(e)}"
    )
```

### 4. Milvus Insert Failure
```python
try:
    insert_result = collection.insert(data)
    collection.load()
except Exception as e:
    return IngestResponse(
        success=False,
        message=f"Vector database insert failed: {str(e)}"
    )
```

---

## ğŸ“Š Production Considerations

### Scaling Factors
- **Concurrent Uploads**: FastAPI async handling ile multiple PDF paralel iÅŸleme
- **Embedding Rate Limits**: OpenAI API quota management
- **Milvus Memory**: Vector count arttÄ±kÃ§a RAM gereksinimi
- **Disk Space**: PDF files + vector indexes

### Optimization Tips
1. **Batch Embeddings**: Birden fazla chunk'Ä± tek API call'da iÅŸle
2. **Async Processing**: Background tasks ile non-blocking upload
3. **Caching**: AynÄ± text chunk'lar iÃ§in embedding cache
4. **Chunking Strategy**: Optimal chunk size tuning (512 tokens ideal)

Bu anlÄ±k yÃ¼kleme sistemi sayesinde kullanÄ±cÄ±lar PDF yÃ¼kler yÃ¼klemez (15 saniye iÃ§inde) hemen sorularÄ±nÄ± sorabilir ve yeni dokÃ¼mandan cevap alabilirler!