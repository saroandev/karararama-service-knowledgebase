# RAG Pipeline Ingestion Flow Documentation

## ğŸ“Š Complete Data Flow: Streamlit â†’ FastAPI â†’ Storage

Bu dokÃ¼mantasyon, bir PDF dosyasÄ±nÄ±n Streamlit UI'dan yÃ¼klenmesinden, iÅŸlenmesi ve depolanmasÄ±na kadar olan tÃ¼m sÃ¼reci detaylÄ± olarak aÃ§Ä±klamaktadÄ±r.

---

## 1ï¸âƒ£ **Streamlit UI - Dosya YÃ¼kleme**

### Lokasyon: `streamlit_app.py` (satÄ±r 26-45)

### Ä°ÅŸlem AdÄ±mlarÄ±:
1. **KullanÄ±cÄ± PDF seÃ§er** 
   - Component: `st.file_uploader()`
   - Kabul edilen format: `type=['pdf']`
   - Return type: `UploadedFile` object

2. **Dosya bilgileri gÃ¶sterilir**
   ```python
   - uploaded_file.name     # String: "dokuman.pdf"
   - uploaded_file.size     # Integer: 1048576 (bytes)
   - uploaded_file.type     # String: "application/pdf"
   ```

3. **Process butonuna basÄ±lÄ±r**
   - HTTP Request hazÄ±rlanÄ±r:
   ```python
   files = {
       "file": (
           uploaded_file.name,    # String: dosya adÄ±
           uploaded_file,         # BytesIO: dosya iÃ§eriÄŸi
           "application/pdf"      # String: MIME type
       )
   }
   ```

4. **API'ye POST isteÄŸi**
   ```python
   response = requests.post(
       "http://localhost:8080/ingest",
       files=files,
       timeout=300  # 5 dakika timeout
   )
   ```

### Ã‡Ä±ktÄ±:
```json
{
    "success": true,
    "document_id": "doc_a3f5b2c8d9e1f4g6",
    "document_title": "dokuman.pdf",
    "chunks_created": 25,
    "processing_time": 12.5,
    "file_hash": "a3f5b2c8d9e1f4g6h7i8j9k0l1m2n3o4",
    "message": "Document successfully ingested with 25 chunks"
}
```

---

## 2ï¸âƒ£ **FastAPI - Dosya AlÄ±mÄ± ve Validasyon**

### Lokasyon: `production_server.py` (satÄ±r 186-209)

### Endpoint: `POST /ingest`

### Ä°ÅŸlem AdÄ±mlarÄ±:

1. **Dosya AlÄ±mÄ±**
   ```python
   file: UploadFile = File(...)
   # UploadFile Ã¶zellikleri:
   # - filename: str
   # - content_type: str
   # - file: SpooledTemporaryFile
   ```

2. **Dosya Tipi Validasyonu**
   ```python
   if not file.filename.lower().endswith('.pdf'):
       raise HTTPException(status_code=400, detail="Only PDF files are supported")
   ```

3. **PDF Binary Data Okuma**
   ```python
   pdf_data = await file.read()  # bytes: PDF binary iÃ§eriÄŸi
   # Ã–rnek: b'%PDF-1.4\n%\xc3\xa4\xc3\xbc...'
   ```

4. **MD5 Hash Hesaplama**
   ```python
   file_hash = hashlib.md5(pdf_data).hexdigest()
   # String: "a3f5b2c8d9e1f4g6h7i8j9k0l1m2n3o4" (32 karakter)
   ```

5. **Document ID OluÅŸturma**
   ```python
   document_id = f"doc_{file_hash[:16]}"
   # String: "doc_a3f5b2c8d9e1f4g6" (ilk 16 karakter kullanÄ±lÄ±r)
   ```

### Data Transformasyonu:
```
UploadFile â†’ bytes â†’ MD5 hash â†’ document_id
```

---

## 3ï¸âƒ£ **MinIO Storage - Dosya Depolama**

### Lokasyon: `production_server.py` (satÄ±r 210-238)

### Ä°ki Bucket'a YÃ¼kleme:

#### A. **rag-docs Bucket** (Chunking iÃ§in)
```python
minio_doc_id = storage_service.upload_pdf(
    file_data=pdf_data,           # bytes
    filename=file.filename,       # String: "dokuman.pdf"
    metadata={
        "document_id": document_id,  # String: "doc_a3f5b2c8d9e1f4g6"
        "file_hash": file_hash       # String: 32 karakter MD5
    }
)
# Return: String - MinIO object ID
```

#### B. **raw-documents Bucket** (Orijinal saklanmasÄ± iÃ§in)
```python
success = storage_service.upload_pdf_to_raw_documents(
    document_id=document_id,
    file_data=pdf_data,
    filename=file.filename,
    metadata={
        "document_id": document_id,
        "file_hash": file_hash,
        "original_filename": file.filename
    }
)
# Return: Boolean - baÅŸarÄ± durumu
```

### MinIO'da Depolama YapÄ±sÄ±:
```
minio/
â”œâ”€â”€ rag-docs/
â”‚   â””â”€â”€ doc_a3f5b2c8d9e1f4g6.pdf
â””â”€â”€ raw-documents/
    â””â”€â”€ doc_a3f5b2c8d9e1f4g6/
        â””â”€â”€ dokuman.pdf
```

---

## 4ï¸âƒ£ **PDF Parsing - Metin Ã‡Ä±karma**

### Lokasyon: `production_server.py` (satÄ±r 240-246)

### Ä°ÅŸlem:
```python
parser = PDFParser()
pages, metadata = parser.extract_text_from_pdf(pdf_data)
```

### Ã‡Ä±ktÄ± Data YapÄ±sÄ±:
```python
# pages: List[Page]
Page = {
    "text": str,           # Sayfa metni
    "page_number": int,    # Sayfa numarasÄ± (1-indexed)
    "metadata": dict       # Ek metadata
}

# metadata: PDFMetadata
PDFMetadata = {
    "title": str,          # PDF baÅŸlÄ±ÄŸÄ±
    "author": str,         # Yazar
    "subject": str,        # Konu
    "creator": str,        # OluÅŸturan uygulama
    "producer": str,       # PDF producer
    "creation_date": str,  # OluÅŸturma tarihi
    "modification_date": str,  # Son deÄŸiÅŸiklik
    "pages": int           # Toplam sayfa sayÄ±sÄ±
}
```

---

## 5ï¸âƒ£ **Text Chunking - Metin ParÃ§alama**

### Lokasyon: `production_server.py` (satÄ±r 248-269)

### Ä°ÅŸlem:
```python
chunks = []
for i, page in enumerate(pages):
    text = page.text.strip()
    if len(text) > 100:  # 100 karakterden kÄ±sa sayfalarÄ± atla
        chunk_id = f"{document_id}_{i:04d}"
        chunk = SimpleChunk(
            chunk_id=chunk_id,
            text=text,
            page_number=page.page_number
        )
        chunks.append(chunk)
```

### Chunk Data YapÄ±sÄ±:
```python
@dataclass
class SimpleChunk:
    chunk_id: str      # "doc_a3f5b2c8d9e1f4g6_0001"
    text: str          # Chunk metni
    page_number: int   # Hangi sayfadan geldiÄŸi
```

---

## 6ï¸âƒ£ **Embedding Generation - VektÃ¶r OluÅŸturma**

### Lokasyon: `production_server.py` (satÄ±r 306-383)

### OpenAI API ile Batch Processing:

```python
# Batch boyutu: 20 chunk
batch_size = 20

for batch_start in range(0, len(chunks), batch_size):
    batch_texts = [chunk.text for chunk in batch_chunks]
    
    # OpenAI API Ã§aÄŸrÄ±sÄ±
    response = openai_client.embeddings.create(
        model='text-embedding-3-small',
        input=batch_texts  # List[str]
    )
    
    # Embedding boyutu kÄ±saltma (1536 â†’ 384)
    batch_embeddings = [data.embedding[:384] for data in response.data]
```

### Embedding Ã–zellikleri:
```python
# Orijinal: 1536 boyut (text-embedding-3-small)
# KÄ±saltÄ±lmÄ±ÅŸ: 384 boyut (Milvus collection ÅŸemasÄ±na uygun)
# Tip: List[float]
# Ã–rnek: [0.023, -0.045, 0.112, ..., 0.089]  # 384 float deÄŸer
```

---

## 7ï¸âƒ£ **MinIO Chunk Storage - Chunk Depolama**

### Lokasyon: `production_server.py` (satÄ±r 385-399)

### Her chunk iÃ§in ayrÄ± JSON dosyasÄ±:
```python
for i, (chunk_id, text) in enumerate(zip(chunk_ids, texts)):
    storage_service.upload_chunk(
        document_id=document_id,      # "doc_a3f5b2c8d9e1f4g6"
        chunk_id=chunk_id,            # "doc_a3f5b2c8d9e1f4g6_0001"
        chunk_text=text,              # Chunk metni
        metadata={
            "page_num": page_nums[i],
            "chunk_index": chunk_indices[i]
        }
    )
```

### MinIO'da Chunk YapÄ±sÄ±:
```
minio/rag-chunks/
â””â”€â”€ doc_a3f5b2c8d9e1f4g6/
    â”œâ”€â”€ doc_a3f5b2c8d9e1f4g6_0000.json
    â”œâ”€â”€ doc_a3f5b2c8d9e1f4g6_0001.json
    â”œâ”€â”€ doc_a3f5b2c8d9e1f4g6_0002.json
    â””â”€â”€ ...
```

### JSON Ä°Ã§eriÄŸi:
```json
{
    "chunk_id": "doc_a3f5b2c8d9e1f4g6_0001",
    "text": "Bu chunk'Ä±n metin iÃ§eriÄŸi...",
    "metadata": {
        "page_num": 1,
        "chunk_index": 0
    }
}
```

---

## 8ï¸âƒ£ **Milvus Vector Storage - VektÃ¶r Depolama**

### Lokasyon: `production_server.py` (satÄ±r 401-447)

### Milvus Collection ÅemasÄ±:
```python
fields = [
    "id",           # VARCHAR(100) - Primary key
    "document_id",  # VARCHAR(100) 
    "chunk_index",  # INT64
    "text",         # VARCHAR(65535)
    "embedding",    # FLOAT_VECTOR(384)
    "metadata"      # JSON
]
```

### Data HazÄ±rlama ve Insert:
```python
# Metadata hazÄ±rlama
combined_metadata = {
    "chunk_id": chunk_id,
    "page_number": page_number,
    "minio_object_path": f"{document_id}/{chunk_id}.json",
    "document_title": document_title,
    "file_hash": file_hash,
    "created_at": timestamp
}

# Batch insert data
data = [
    ids,                 # List[str]
    document_ids,        # List[str]
    chunk_indices,       # List[int]
    texts,              # List[str]
    embeddings,          # List[List[float]]
    combined_metadata    # List[dict]
]

insert_result = collection.insert(data)
collection.load()  # Immediate search iÃ§in yeniden yÃ¼kle
```

---

## 9ï¸âƒ£ **Response - Ä°stemciye DÃ¶nÃ¼ÅŸ**

### Final Response YapÄ±sÄ±:
```python
IngestResponse = {
    "success": True,
    "document_id": "doc_a3f5b2c8d9e1f4g6",
    "document_title": "dokuman.pdf",
    "chunks_created": 25,
    "processing_time": 12.5,  # saniye
    "file_hash": "a3f5b2c8d9e1f4g6h7i8j9k0l1m2n3o4",
    "message": "Document successfully ingested with 25 chunks"
}
```

---

## ğŸ“ˆ **Performans Metrikleri**

### Tipik Ä°ÅŸlem SÃ¼releri:
- **PDF Parsing**: ~0.5-2 saniye (sayfa sayÄ±sÄ±na gÃ¶re)
- **Chunking**: ~0.1-0.5 saniye
- **Embedding Generation**: ~2-10 saniye (chunk sayÄ±sÄ±na gÃ¶re)
- **MinIO Upload**: ~1-3 saniye
- **Milvus Insert**: ~0.5-2 saniye
- **Toplam**: ~5-20 saniye (dosya boyutuna gÃ¶re)

### Batch Processing OptimizasyonlarÄ±:
- OpenAI Embedding: 20 chunk/batch
- Milvus Insert: Tek seferde tÃ¼m chunks
- MinIO Upload: Paralel iÅŸlem mÃ¼mkÃ¼n

---

## ğŸ” **Duplicate Check - Tekrar KontrolÃ¼**

### Lokasyon: `production_server.py` (satÄ±r 276-291)

```python
# Milvus'ta aynÄ± document_id var mÄ± kontrol et
search_existing = collection.query(
    expr=f'document_id == "{document_id}"',
    output_fields=['id'],
    limit=1
)

if search_existing:
    # Dosya zaten var, iÅŸlemi durdur
    return IngestResponse(
        success=False,
        message="Document already exists in database"
    )
```

---

## ğŸš¨ **Error Handling - Hata YÃ¶netimi**

### Her AÅŸamada Try-Catch:

1. **MinIO Upload HatasÄ±**: Log kaydedilir, iÅŸlem devam eder
2. **PDF Parse HatasÄ±**: HTTPException(500) fÄ±rlatÄ±lÄ±r
3. **Embedding HatasÄ±**: HTTPException(500) fÄ±rlatÄ±lÄ±r
4. **Milvus Insert HatasÄ±**: Response'da hata dÃ¶ndÃ¼rÃ¼lÃ¼r

### Retry Mechanism:
```python
@retry_with_backoff(max_retries=3)
async def ingest_document(file: UploadFile):
    # 3 deneme hakkÄ± ile
```

---

## ğŸ“Š **Data Flow Diagram**

```mermaid
graph TD
    A[Streamlit UI] -->|PDF Upload| B[FastAPI /ingest]
    B --> C{File Validation}
    C -->|Valid PDF| D[Generate Hash & ID]
    C -->|Invalid| E[Error 400]
    
    D --> F[MinIO Upload]
    F --> G[PDF Parser]
    G --> H[Text Chunking]
    H --> I{Duplicate Check}
    
    I -->|Exists| J[Return: Already Exists]
    I -->|New| K[Generate Embeddings]
    
    K --> L[Save Chunks to MinIO]
    L --> M[Insert to Milvus]
    M --> N[Return Success Response]
    
    style A fill:#e1f5fe
    style B fill:#fff9c4
    style M fill:#c8e6c9
    style N fill:#a5d6a7
```

---

## ğŸ“ **Ã–zet**

Bu pipeline, PDF dosyalarÄ±nÄ±:
1. **AlÄ±r ve doÄŸrular** (FastAPI)
2. **Hashler ve ID oluÅŸturur** (MD5)
3. **Depolar** (MinIO - 2 bucket)
4. **Parse eder** (PyMuPDF)
5. **Chunk'lara bÃ¶ler** (Simple chunking)
6. **VektÃ¶rleÅŸtirir** (OpenAI embeddings)
7. **Chunk'larÄ± saklar** (MinIO JSON)
8. **VektÃ¶r DB'ye ekler** (Milvus)
9. **Sonucu dÃ¶ndÃ¼rÃ¼r** (JSON response)

TÃ¼m sÃ¼reÃ§ ortalama **5-20 saniye** sÃ¼rer ve **fault-tolerant** yapÄ±dadÄ±r.