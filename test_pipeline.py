#!/usr/bin/env python3
"""
Step-by-step RAG pipeline tester
Milvus olmadan her adÄ±mÄ± test ederiz
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.parse import pdf_parser
from app.chunk import DocumentBasedChunker
from app.embed import EmbeddingGenerator
from app.storage import storage
import json
from pathlib import Path

def test_pdf_parsing(pdf_path: str):
    """Test PDF parsing"""
    print("ğŸ” Step 1: PDF Parsing Test")
    print("-" * 50)
    
    try:
        with open(pdf_path, 'rb') as f:
            pdf_data = f.read()
        
        print(f"ğŸ“„ PDF dosyasÄ± okundu: {len(pdf_data)} bytes")
        
        # Parse PDF
        pages, metadata = pdf_parser.extract_text_from_pdf(pdf_data)
        
        print(f"ğŸ“‘ Sayfa sayÄ±sÄ±: {len(pages)}")
        print(f"ğŸ“Š Metadata: {metadata.title or 'BaÅŸlÄ±k yok'}")
        print(f"ğŸ“ Ä°lk sayfa (ilk 200 karakter):")
        if pages:
            print(pages[0].text[:200] + "...")
        
        return pages, metadata
        
    except Exception as e:
        print(f"âŒ PDF parsing hatasÄ±: {e}")
        return None, None

def test_document_chunking(pages, document_id="test_doc_001"):
    """Test document-based chunking"""
    print("\nğŸ”§ Step 2: Document-Based Chunking Test")
    print("-" * 50)
    
    try:
        chunker = DocumentBasedChunker(chunk_size=512, chunk_overlap=50)
        chunks = chunker.chunk_by_document(pages, document_id)
        
        print(f"ğŸ“¦ OluÅŸturulan chunk sayÄ±sÄ±: {len(chunks)}")
        
        for i, chunk in enumerate(chunks[:3]):  # Ä°lk 3 chunk'Ä± gÃ¶ster
            print(f"\nğŸ“‹ Chunk {i+1}:")
            print(f"   ID: {chunk.chunk_id}")
            print(f"   Sayfa: {chunk.metadata.get('page_number', 'N/A')}")
            print(f"   Token sayÄ±sÄ±: {chunk.token_count}")
            print(f"   Metin (ilk 150 karakter): {chunk.text[:150]}...")
        
        return chunks
        
    except Exception as e:
        print(f"âŒ Chunking hatasÄ±: {e}")
        return None

def test_embedding_generation(chunks):
    """Test embedding generation"""
    print("\nğŸ¤– Step 3: Embedding Generation Test")
    print("-" * 50)
    
    try:
        # Initialize embedding generator
        embedder = EmbeddingGenerator()
        
        print(f"ğŸ§  Model: {embedder.model_name}")
        print(f"ğŸ“ Dimension: {embedder.dimension}")
        print(f"ğŸ”§ Device: {embedder.device}")
        
        # Test with first few chunks
        test_chunks = chunks[:3] if len(chunks) >= 3 else chunks
        chunk_texts = [chunk.text for chunk in test_chunks]
        
        print(f"\nâš™ï¸ {len(chunk_texts)} chunk iÃ§in embedding Ã¼retiliyor...")
        embeddings = embedder.generate_embeddings_batch(chunk_texts, show_progress=True)
        
        print(f"âœ… Embedding Ã¼retimi tamamlandÄ±!")
        print(f"ğŸ“Š Embedding shape: {len(embeddings)} x {len(embeddings[0]) if embeddings else 0}")
        
        # Ä°lk embedding'in ilk 10 deÄŸerini gÃ¶ster
        if embeddings:
            print(f"ğŸ” Ä°lk embedding (ilk 10 deÄŸer): {embeddings[0][:10]}")
        
        return embeddings
        
    except Exception as e:
        print(f"âŒ Embedding hatasÄ±: {e}")
        return None

def test_storage_operations(chunks, embeddings, document_id="test_doc_001"):
    """Test storage operations (MinIO simulation)"""
    print("\nğŸ’¾ Step 4: Storage Test (MinIO Simulation)")
    print("-" * 50)
    
    try:
        # Sadece dosya sistemine kaydet (MinIO yerine)
        output_dir = Path("./test_output")
        output_dir.mkdir(exist_ok=True)
        
        # Save chunks
        chunks_data = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            chunk_data = {
                "chunk_id": chunk.chunk_id,
                "text": chunk.text,
                "metadata": chunk.metadata,
                "token_count": chunk.token_count,
                "char_count": chunk.char_count,
                "embedding": embedding.tolist()  # JSON iÃ§in list'e Ã§evir
            }
            chunks_data.append(chunk_data)
        
        # Save to JSON
        output_file = output_dir / f"{document_id}_chunks.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {len(chunks_data)} chunk kaydedildi: {output_file}")
        print(f"ğŸ“ Dosya boyutu: {output_file.stat().st_size / 1024:.2f} KB")
        
        return output_file
        
    except Exception as e:
        print(f"âŒ Storage hatasÄ±: {e}")
        return None

def main():
    print("ğŸš€ RAG Pipeline Step-by-Step Test")
    print("=" * 60)
    
    # PDF dosyasÄ± yolu - bunu gÃ¼ncelleyin
    pdf_path = input("ğŸ“ PDF dosya yolunu girin: ").strip()
    
    if not os.path.exists(pdf_path):
        print(f"âŒ Dosya bulunamadÄ±: {pdf_path}")
        return
    
    # Step 1: PDF Parsing
    pages, metadata = test_pdf_parsing(pdf_path)
    if not pages:
        return
    
    # Step 2: Document Chunking
    chunks = test_document_chunking(pages)
    if not chunks:
        return
    
    # Step 3: Embedding Generation
    embeddings = test_embedding_generation(chunks)
    if not embeddings:
        return
    
    # Step 4: Storage Test
    output_file = test_storage_operations(chunks, embeddings)
    
    print("\n" + "=" * 60)
    print("âœ… TÃ¼m testler tamamlandÄ±!")
    print(f"ğŸ“Š SonuÃ§: {len(pages)} sayfa â†’ {len(chunks)} chunk â†’ {len(embeddings)} embedding")
    if output_file:
        print(f"ğŸ’¾ Ã‡Ä±ktÄ± dosyasÄ±: {output_file}")
    print("=" * 60)

if __name__ == "__main__":
    main()