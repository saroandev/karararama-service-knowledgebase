# ğŸ” QUERY PIPELINE - KullanÄ±cÄ± Sorgusu Ä°ÅŸleme ve Cevap Ãœretme AkÄ±ÅŸÄ±

## ğŸ¯ Genel BakÄ±ÅŸ

Query Pipeline, kullanÄ±cÄ±nÄ±n sorduÄŸu soruyu alÄ±p, ilgili dokÃ¼man parÃ§alarÄ±nÄ± bulup, kaynak gÃ¶stererek akÄ±llÄ± cevaplar Ã¼reten end-to-end sÃ¼reÃ§tir. Bu pipeline semantik arama, reranking ve LLM-based cevap Ã¼retimi iÃ§erir.

## ğŸ”„ Pipeline AkÄ±ÅŸ DiyagramÄ±

```mermaid
flowchart TD
    Start([KullanÄ±cÄ± Soru Sorar]) --> Input[1. Query Input Validation]
    Input --> EmbedQuery[2. Generate Query Embedding]
    EmbedQuery --> VectorSearch[3. Vector Similarity Search]
    VectorSearch --> LoadChunks[4. Load Chunks from MinIO]
    LoadChunks --> Rerank{5. Reranking?}
    
    Rerank -->|Yes| RerankChunks[5a. Rerank with BGE Model]
    Rerank -->|No| PrepareContext[6. Prepare Context]
    RerankChunks --> PrepareContext
    
    PrepareContext --> GenerateLLM[7. LLM Response Generation]
    GenerateLLM --> FormatResponse[8. Format Response with Sources]
    FormatResponse --> Return[9. Return to User]
    Return --> End([End])

    style Start fill:#4ade80
    style End fill:#f87171
    style Rerank fill:#fbbf24
    style GenerateLLM fill:#60a5fa
```

## ğŸ—ï¸ Sistem Mimarisi

```mermaid
graph TB
    subgraph User["ğŸ‘¤ User Interface"]
        Streamlit[Streamlit Chat UI<br/>Port: 8501]
    end
    
    subgraph API["âš¡ API Processing"]
        FastAPI[FastAPI Server<br/>Port: 8080]
        Embedder[Embedding Service]
        Retriever[Retrieval Service]
        Generator[Generation Service]
    end
    
    subgraph Storage["ğŸ’¾ Data Sources"]
        Milvus[Milvus Vector DB<br/>19530]
        MinIO[MinIO Object Store<br/>9000]
    end
    
    subgraph AI["ğŸ¤– AI Services"]
        OpenAIEmbed[OpenAI Embeddings<br/>text-embedding-3-small]
        OpenAILLM[OpenAI GPT-4<br/>gpt-4o-mini]
        BGEReranker[BGE Reranker<br/>BAAI/bge-reranker-v2-m3]
    end
    
    Streamlit -->|POST /query| FastAPI
    FastAPI --> Embedder
    Embedder --> OpenAIEmbed
    FastAPI --> Retriever
    Retriever --> Milvus
    Retriever --> MinIO
    Retriever --> BGEReranker
    FastAPI --> Generator
    Generator --> OpenAILLM
    
    style User fill:#e0f2fe
    style API fill:#fef3c7
    style Storage fill:#dcfce7
    style AI fill:#fce7f3
```

## ğŸ“‹ DetaylÄ± AdÄ±m AÃ§Ä±klamalarÄ±

### ADIM 1: Query Input Validation
**Endpoint:** `POST /query`
**Component:** `streamlit_app.py` â†’ `production_server.py`

```python
# Streamlit tarafÄ± (streamlit_app.py:150-165)
def send_query(question: str):
    """Send query to backend API"""
    try:
        response = requests.post(
            f"{API_BASE_URL}/query",
            json={
                "question": question,
                "top_k": 5,  # KaÃ§ chunk getirilecek
                "use_reranker": True  # Reranking kullanÄ±lsÄ±n mÄ±
            },
            timeout=30
        )
        return response.json()
    except Exception as e:
        st.error(f"Query failed: {str(e)}")
```

**FastAPI Route Handler:**
```python
# production_server.py:200-210
@app.post("/query", response_model=QueryResponse)
async def query_documents(request: QueryRequest):
    start_time = datetime.datetime.now()
    
    # Input validation
    if not request.question or len(request.question.strip()) < 3:
        raise HTTPException(
            status_code=400,
            detail="Question must be at least 3 characters"
        )
    
    # Normalize parameters
    top_k = request.top_k or 5
    use_reranker = request.use_reranker or False
    
    logger.info(f"Processing query: {request.question[:50]}...")
```

**Request Model:**
```python
class QueryRequest(BaseModel):
    question: str
    top_k: Optional[int] = 5
    use_reranker: Optional[bool] = True
    filters: Optional[Dict] = None  # For metadata filtering
```

---

### ADIM 2: Generate Query Embedding
**Ä°ÅŸlem:** KullanÄ±cÄ± sorusunu vektÃ¶re Ã§evirme
**Component:** `production_server.py:215-230` â†’ OpenAI API

```python
# Generate embedding for the query
from openai import OpenAI
import numpy as np

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# Step 1: Clean and prepare query
clean_query = request.question.strip()
clean_query = ' '.join(clean_query.split())  # Normalize whitespace

# Step 2: Generate embedding via OpenAI
try:
    embedding_response = client.embeddings.create(
        model='text-embedding-3-small',
        input=clean_query,
        encoding_format="float"
    )
    
    # Extract 1536-dimensional vector
    query_embedding = embedding_response.data[0].embedding
    query_vector = np.array(query_embedding, dtype=np.float32)
    
    logger.info(f"Generated query embedding: dim={len(query_vector)}")
    
except Exception as e:
    logger.error(f"Embedding generation failed: {e}")
    raise HTTPException(
        status_code=500,
        detail="Failed to generate query embedding"
    )
```

**Embedding Details:**
```python
{
    "model": "text-embedding-3-small",
    "dimensions": 1536,
    "input_tokens": 15,  # Ã–rnek
    "processing_time": 0.08  # seconds
}
```

---

### ADIM 3: Vector Similarity Search in Milvus
**Ä°ÅŸlem:** En benzer chunk'larÄ± bulma
**Component:** `production_server.py:235-270` â†’ Milvus

```python
from pymilvus import connections, Collection
import json

# Connect to Milvus
connections.connect('default', host='localhost', port='19530')
collection = Collection('rag_production_v1')

# Ensure collection is loaded
collection.load()

# Search parameters
search_params = {
    "metric_type": "COSINE",  # Cosine similarity
    "params": {
        "nprobe": 128,  # Number of clusters to search
        "ef": 256  # Search accuracy parameter
    }
}

# Execute vector search
search_results = collection.search(
    data=[query_vector.tolist()],  # Query vector
    anns_field="embedding",  # Vector field name
    param=search_params,
    limit=top_k * 2,  # Get extra for reranking
    output_fields=[
        "chunk_id",
        "document_id", 
        "document_title",
        "minio_path",
        "page_num",
        "chunk_index"
    ]
)

# Process search results
chunks_data = []
for hits in search_results:
    for hit in hits:
        chunk_info = {
            "chunk_id": hit.entity.get("chunk_id"),
            "document_id": hit.entity.get("document_id"),
            "document_title": hit.entity.get("document_title"),
            "minio_path": hit.entity.get("minio_path"),
            "page_num": hit.entity.get("page_num"),
            "score": hit.score,  # Cosine similarity score [0, 1]
            "distance": hit.distance  # Distance metric
        }
        chunks_data.append(chunk_info)

logger.info(f"Found {len(chunks_data)} similar chunks")
```

**Milvus Search Metrics:**
```python
{
    "search_latency": 0.015,  # seconds
    "chunks_retrieved": 10,
    "similarity_range": [0.75, 0.95],  # Min-max scores
    "index_type": "IVF_FLAT",
    "metric": "COSINE"
}
```

---

### ADIM 4: Load Chunk Texts from MinIO
**Ä°ÅŸlem:** Chunk metinlerini MinIO'dan yÃ¼kleme
**Component:** `production_server.py:275-310` â†’ MinIO

```python
from minio import Minio
import json
from io import BytesIO

# MinIO client
minio_client = Minio(
    "localhost:9000",
    access_key="minioadmin",
    secret_key="minioadmin",
    secure=False
)

# Load chunk texts from MinIO
for chunk in chunks_data:
    try:
        # Get chunk JSON from MinIO
        minio_path = chunk["minio_path"]
        
        # Fetch object
        response = minio_client.get_object(
            bucket_name="chunks",
            object_name=minio_path
        )
        
        # Parse JSON content
        chunk_json = json.loads(response.read().decode('utf-8'))
        
        # Add text to chunk data
        chunk["text"] = chunk_json["text"]
        chunk["metadata"] = chunk_json.get("metadata", {})
        
        # Close MinIO response
        response.close()
        response.release_conn()
        
    except Exception as e:
        logger.error(f"Failed to load chunk {minio_path}: {e}")
        chunk["text"] = "[Chunk loading failed]"

logger.info(f"Loaded texts for {len(chunks_data)} chunks")
```

**MinIO Fetch Pattern:**
```python
# Parallel fetching for performance
from concurrent.futures import ThreadPoolExecutor

def fetch_chunk(minio_path):
    # MinIO fetch logic
    pass

with ThreadPoolExecutor(max_workers=5) as executor:
    futures = [executor.submit(fetch_chunk, c["minio_path"]) for c in chunks_data]
    results = [f.result() for f in futures]
```

---

### ADIM 5: Reranking (Optional)
**Ä°ÅŸlem:** BGE reranker ile chunk'larÄ± yeniden sÄ±ralama
**Component:** `production_server.py:315-350` â†’ BGE Reranker

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

if use_reranker and len(chunks_data) > 1:
    # Load BGE reranker model
    model_name = "BAAI/bge-reranker-v2-m3"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    model.eval()
    
    # Prepare pairs for reranking
    pairs = []
    for chunk in chunks_data:
        pairs.append([clean_query, chunk["text"]])
    
    # Tokenize and get scores
    with torch.no_grad():
        inputs = tokenizer(
            pairs,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
        
        scores = model(**inputs).logits.squeeze(-1)
        scores = torch.nn.functional.sigmoid(scores).numpy()
    
    # Add reranking scores
    for i, chunk in enumerate(chunks_data):
        chunk["rerank_score"] = float(scores[i])
        chunk["original_rank"] = i + 1
    
    # Sort by reranking score
    chunks_data = sorted(
        chunks_data, 
        key=lambda x: x["rerank_score"], 
        reverse=True
    )[:top_k]
    
    logger.info(f"Reranked to top {len(chunks_data)} chunks")
```

**Reranking Impact:**
```python
{
    "before_reranking": {
        "top_chunk_score": 0.85,
        "order": [1, 2, 3, 4, 5]
    },
    "after_reranking": {
        "top_chunk_score": 0.92,
        "order": [3, 1, 5, 2, 4],  # Reordered
        "improvement": "+7%"
    }
}
```

---

### ADIM 6: Prepare Context for LLM
**Ä°ÅŸlem:** LLM iÃ§in context hazÄ±rlama
**Component:** `production_server.py:355-380`

```python
# Build context from retrieved chunks
context_parts = []
source_references = []

for i, chunk in enumerate(chunks_data, 1):
    # Format chunk for context
    context_part = f"""
[Kaynak {i}]
DokÃ¼man: {chunk['document_title']}
Sayfa: {chunk['page_num']}
Ä°Ã§erik: {chunk['text']}
---
"""
    context_parts.append(context_part)
    
    # Prepare source reference
    source_ref = {
        "index": i,
        "document_title": chunk['document_title'],
        "page_number": chunk['page_num'],
        "score": chunk.get('rerank_score', chunk['score']),
        "text_preview": chunk['text'][:200] + "..."
    }
    source_references.append(source_ref)

# Combine context
full_context = "\n".join(context_parts)

# Calculate token usage
estimated_tokens = len(full_context.split()) * 1.3  # Rough estimate

logger.info(f"Context prepared: {len(context_parts)} sources, ~{estimated_tokens} tokens")
```

**Context Structure:**
```
Total Context: 2500 tokens
â”œâ”€â”€ System Prompt: 150 tokens
â”œâ”€â”€ User Question: 50 tokens
â”œâ”€â”€ Retrieved Chunks: 2000 tokens (5 x 400)
â””â”€â”€ Format Instructions: 300 tokens
```

---

### ADIM 7: LLM Response Generation
**Ä°ÅŸlem:** GPT-4 ile cevap Ã¼retme
**Component:** `production_server.py:385-420` â†’ OpenAI GPT-4

```python
from openai import OpenAI

# System prompt for RAG
system_prompt = """Sen yardÄ±mcÄ± bir asistansÄ±n. Sana verilen kaynaklara dayanarak sorularÄ± cevaplÄ±yorsun.

KURALLAR:
1. SADECE verilen kaynaklardaki bilgileri kullan
2. Her bilgi iÃ§in kaynak numarasÄ±nÄ± belirt [Kaynak X]
3. Kaynaklarda olmayan bilgi iÃ§in "Verilen kaynaklarda bu bilgi bulunmamaktadÄ±r" de
4. CevabÄ±nÄ± TÃ¼rkÃ§e ver
5. Net, aÃ§Ä±k ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ cevaplar ver
"""

# Build messages
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": f"""
Soru: {clean_query}

Kaynaklar:
{full_context}

LÃ¼tfen yukarÄ±daki kaynaklara dayanarak soruyu cevapla.
"""}
]

# Generate response with GPT-4
try:
    llm_response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.3,  # Lower for more factual responses
        max_tokens=1000,
        top_p=0.9,
        frequency_penalty=0.0,
        presence_penalty=0.0
    )
    
    answer = llm_response.choices[0].message.content
    
    # Extract usage statistics
    usage_stats = {
        "prompt_tokens": llm_response.usage.prompt_tokens,
        "completion_tokens": llm_response.usage.completion_tokens,
        "total_tokens": llm_response.usage.total_tokens,
        "model": "gpt-4o-mini"
    }
    
    logger.info(f"LLM response generated: {usage_stats['total_tokens']} tokens")
    
except Exception as e:
    logger.error(f"LLM generation failed: {e}")
    answer = "ÃœzgÃ¼nÃ¼m, cevap Ã¼retilirken bir hata oluÅŸtu."
```

**LLM Parameters Explained:**
```python
{
    "temperature": 0.3,  # Creativity vs Factuality (0=factual, 1=creative)
    "max_tokens": 1000,  # Maximum response length
    "top_p": 0.9,  # Nucleus sampling (diversity)
    "model": "gpt-4o-mini",  # Fast, cost-effective model
    "response_time": 1.5  # seconds average
}
```

---

### ADIM 8: Format Response with Sources
**Ä°ÅŸlem:** CevabÄ± kaynaklarla formatlama
**Component:** `production_server.py:425-450`

```python
# Calculate processing time
processing_time = (datetime.datetime.now() - start_time).total_seconds()

# Build final response
response = QueryResponse(
    answer=answer,
    sources=source_references,
    processing_time=processing_time,
    metadata={
        "chunks_retrieved": len(chunks_data),
        "reranking_used": use_reranker,
        "tokens_used": usage_stats["total_tokens"],
        "model_used": "gpt-4o-mini",
        "embedding_model": "text-embedding-3-small",
        "search_latency": search_latency,
        "llm_latency": llm_latency
    }
)

logger.info(f"Query completed in {processing_time:.2f}s")

return response
```

**Response Structure:**
```json
{
    "answer": "Posta gezici personeline verilecek harcÄ±rah miktarÄ±, gÃ¶rev yapÄ±lan bÃ¶lgeye ve mesafeye gÃ¶re belirlenir. [Kaynak 1] Madde 5'e gÃ¶re, il iÃ§i gÃ¶revlerde gÃ¼nlÃ¼k 150 TL, il dÄ±ÅŸÄ± gÃ¶revlerde ise 250 TL harcÄ±rah Ã¶denir. [Kaynak 2]",
    "sources": [
        {
            "index": 1,
            "document_title": "Posta Gezici Personeli HarcÄ±rah TÃ¼zÃ¼ÄŸÃ¼",
            "page_number": 3,
            "score": 0.92,
            "text_preview": "MADDE 5 - HarcÄ±rah miktarlarÄ±..."
        },
        {
            "index": 2,
            "document_title": "Posta Gezici Personeli HarcÄ±rah TÃ¼zÃ¼ÄŸÃ¼",
            "page_number": 7,
            "score": 0.88,
            "text_preview": "Ä°l dÄ±ÅŸÄ± gÃ¶revlendirmelerde..."
        }
    ],
    "processing_time": 2.3,
    "metadata": {
        "chunks_retrieved": 5,
        "reranking_used": true,
        "tokens_used": 3250,
        "model_used": "gpt-4o-mini"
    }
}
```

---

### ADIM 9: Return Response to User
**Ä°ÅŸlem:** Streamlit UI'da gÃ¶sterme
**Component:** `streamlit_app.py:170-200`

```python
# Display in Streamlit chat interface
def display_response(response):
    """Display the query response in chat UI"""
    
    # Add to chat history
    st.session_state.messages.append({
        "role": "assistant",
        "content": response["answer"],
        "sources": response.get("sources", [])
    })
    
    # Display the answer
    with st.chat_message("assistant"):
        st.markdown(response["answer"])
        
        # Display sources if available
        if response.get("sources"):
            st.markdown("### ğŸ“š Kaynaklar")
            for source in response["sources"]:
                with st.expander(
                    f"Kaynak {source['index']}: {source['document_title']} "
                    f"(Sayfa {source['page_number']}, Skor: {source['score']:.2f})"
                ):
                    st.text(source["text_preview"])
        
        # Show processing stats
        with st.expander("ğŸ“Š Ä°ÅŸlem DetaylarÄ±"):
            st.json({
                "Ä°ÅŸlem SÃ¼resi": f"{response['processing_time']:.2f} saniye",
                "Taranan Chunk": response['metadata']['chunks_retrieved'],
                "KullanÄ±lan Token": response['metadata']['tokens_used'],
                "Model": response['metadata']['model_used']
            })
```

---

## ğŸ” Error Handling & Edge Cases

### Common Error Scenarios

```python
# 1. No relevant chunks found
if not chunks_data or all(c["score"] < 0.5 for c in chunks_data):
    return QueryResponse(
        answer="ÃœzgÃ¼nÃ¼m, sorunuzla ilgili kaynaklarda bilgi bulunamadÄ±.",
        sources=[],
        processing_time=processing_time,
        metadata={"status": "no_relevant_content"}
    )

# 2. OpenAI API rate limit
except RateLimitError as e:
    logger.warning(f"Rate limit hit: {e}")
    # Implement exponential backoff
    await asyncio.sleep(2 ** retry_count)
    # Retry or fallback to cached response

# 3. Milvus connection failure
except MilvusException as e:
    logger.error(f"Milvus search failed: {e}")
    # Fallback to MinIO full-text search
    return fallback_text_search(query)

# 4. Token limit exceeded
if estimated_tokens > 3500:
    # Truncate context to fit
    chunks_data = chunks_data[:3]  # Use only top 3 chunks
    logger.warning("Context truncated due to token limit")
```

---

## ğŸ“Š Performance Metrics & Optimization

### Typical Query Processing Times

| Stage | Time (avg) | Details | Optimization |
|-------|------------|---------|--------------|
| Input Validation | 0.01s | Basic checks | Cached |
| Query Embedding | 0.08s | OpenAI API | Batch queries |
| Vector Search | 0.02s | Milvus search | Index optimization |
| Load Chunks | 0.15s | MinIO fetch | Connection pooling |
| Reranking | 0.30s | BGE model | GPU acceleration |
| LLM Generation | 1.50s | GPT-4 API | Model selection |
| Response Format | 0.01s | JSON building | - |
| **Total** | **~2.07s** | End-to-end | - |

### Optimization Strategies

```python
# 1. Caching frequently asked questions
from functools import lru_cache

@lru_cache(maxsize=100)
def get_cached_embedding(query: str):
    # Cache query embeddings
    pass

# 2. Parallel processing
async def parallel_chunk_loading(chunk_paths):
    tasks = [load_chunk_async(path) for path in chunk_paths]
    return await asyncio.gather(*tasks)

# 3. Connection pooling
minio_pool = MinioConnectionPool(max_connections=10)
milvus_pool = MilvusConnectionPool(max_connections=5)

# 4. Smart chunking for context
def optimize_context(chunks, max_tokens=3000):
    # Intelligently select most relevant parts
    optimized = []
    token_count = 0
    for chunk in chunks:
        chunk_tokens = len(chunk["text"].split()) * 1.3
        if token_count + chunk_tokens < max_tokens:
            optimized.append(chunk)
            token_count += chunk_tokens
    return optimized
```

---

## ğŸš€ Advanced Features

### 1. Hybrid Search (Vector + Keyword)
```python
# Combine vector similarity with BM25 keyword search
vector_results = milvus_search(query_embedding)
keyword_results = elasticsearch_search(query_text)
hybrid_results = merge_and_rerank(vector_results, keyword_results)
```

### 2. Query Expansion
```python
# Expand user query with synonyms and related terms
expanded_query = query_expander.expand(original_query)
# Example: "maaÅŸ" â†’ "maaÅŸ, Ã¼cret, aylÄ±k, Ã¶zlÃ¼k haklarÄ±"
```

### 3. Multi-turn Conversation
```python
# Maintain conversation context
conversation_history = st.session_state.get("history", [])
contextualized_query = build_context_aware_query(
    current_query, 
    conversation_history
)
```

### 4. Streaming Responses
```python
# Stream LLM responses for better UX
async def stream_response():
    async for chunk in llm.astream(messages):
        yield chunk.content
```

---

## ğŸ“Œ Summary & Key Takeaways

### Query Pipeline Ã–zeti

```mermaid
graph LR
    A[User Query] --> B[Embedding]
    B --> C[Vector Search]
    C --> D[Load Chunks]
    D --> E[Reranking]
    E --> F[LLM Generation]
    F --> G[Response]
    
    style A fill:#f9f9f9
    style G fill:#d4edda
```

### Kritik BaÅŸarÄ± FaktÃ¶rleri

1. **HÄ±z Optimizasyonu**
   - Query embedding caching
   - Parallel chunk loading
   - Connection pooling
   - Smart context truncation

2. **DoÄŸruluk Ä°yileÅŸtirmeleri**
   - Semantic search (vector)
   - Reranking with BGE
   - Source attribution
   - Context relevance filtering

3. **KullanÄ±cÄ± Deneyimi**
   - <2.5s response time
   - Clear source citations
   - Structured answers
   - Error recovery

4. **Sistem GÃ¼venilirliÄŸi**
   - Fallback mechanisms
   - Rate limit handling
   - Connection retry logic
   - Graceful degradation

### Data Flow Summary

```
â“ User Question (15 tokens)
    â†“ Embed
ğŸ”¢ Query Vector (1536d)
    â†“ Search
ğŸ“š 10 Chunks Retrieved
    â†“ Rerank
ğŸ“– 5 Best Chunks
    â†“ Context
ğŸ“ 2500 Token Context
    â†“ LLM
ğŸ’¬ Answer (500 tokens)
    â†“
âœ… Response with Sources
```

### Performance Targets

- âš¡ **Response Time**: <2.5 seconds
- ğŸ¯ **Accuracy**: >90% relevant results
- ğŸ“Š **Token Efficiency**: <4000 tokens/query
- ğŸ”„ **Throughput**: 100+ queries/minute
- ğŸ’¾ **Cache Hit Rate**: >30%

Bu pipeline sayesinde:
- âœ… HÄ±zlÄ± ve doÄŸru cevaplar Ã¼retilir
- âœ… Her cevap kaynak gÃ¶sterir
- âœ… Context-aware responses
- âœ… Scalable architecture
- âœ… Production-ready error handling